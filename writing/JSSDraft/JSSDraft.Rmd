---
author:
  - name: Sarah Koehler
    affiliation: Colorado State University
    address: >
      Lake Street
      Fort Collins, CO
    email: sakoehler7@gmail.com
  - name: G. Brooke Anderson
    affiliation: Colorado State University
    address: >
      Lake Street
      Fort Collins, CO
    email: brooke.anderson@colostate.edu
title:
  formatted: "\\pkg{eesim}: An R Package to Simulate Environmental Epidemiology Time Series for Simulation Studies and Power Analyses"
  # If you use tex in the formatted title, also supply version without
  plain:     "eesim: An R Package to Simulate Environmental Epidemiology Time Series for Simulation Studies and Power Analyses"
  # For running headers, if needed
  short:     "\\pkg{eesim}: Simulations for Environmental Epidemiology"
abstract: >
  Simulation studies are important in environmental epidemiology research on air pollution, temperature, and other exposures. For example, studies have simulated time series of environmental exposure and health outcome data to compare time series and case-crossover methods; explore models to test for mortality-displacement; investigate the impacts of measurement error and spatial misalignment of exposure and outcome measurements; and search for confounding by other time-varying covariates. 
  
  Developing the code to simulate environmental time series for these types of studies can be time consuming, and simulation methods have varied across previous simulation studies, complicating the comparison of results across different studies. Here, we present an R package that automates simulation of environmental time series for such simulations, offering both sensible defaults for each stage of the simulation process and also the option to customize any of the simulation steps while using defaults for any steps that do not require customization. This packge can be used to aid simulation studies to explore environmental epidemiology models relating acute health outcomes and daily environmental exposures; it can also be used to estimate the power of studies when planning or proposing future research.
keywords:
  # at least one keyword must be supplied
  formatted: [R, environmental epidemiology, simulation studies, power analysis, air pollution]
  plain:     [R, environmental epidemiology, simulation studies, power analysis, air pollution]
preamble: >
  \usepackage{amsmath}
output:
  rticles::jss_article:
    citation_package: natbib
bibliography: mybibfile.bib
---

# Introduction

## Environmental time series studies and why they're important

## Using simulated time series for power analysis

Simulation studies can be used to determine effective sample size or power when planning or proposing future research studies. This method can be particularly useful in cases where modeling of the study data will be particularly complex and when the strong assumptions of classical, analytical power analysis are questionable \citep{Bellan2015, Johnson2015, Burton2009, Alfons2010, Beaujean2014}. Simulation studies can often be used to generate power analyses under a more realistic set of assumptions \cite{Burton2009, Beaujean2014}.

For example, \citet{Bellan2015} used a simulation study to determine the power and statistical validity of two potential Ebola vaccine trials, a randomized controlled trial and a stepped-wedge cluster trial. Simulation was useful in this case because the trial would be conducting in a population with clustering by geographical district and with a temporally changing hazard of Ebola infection over the trial period \citep{Bellan2015}.

\citet{Johnson2015} give an example of using simulation for a power analysis of a study of numbers of ticks on grouse chicks in a study design with multi-level clustering of the study population, both by geographic area and, within that, by brood.

\citet{Burton2009} used simulation to investigate the power of case-control studies of genetic associations for chronic diseases.

\citet{Schoemann2014} demonstrate how to use simulation studies to assess power for study designs with planned missing data. 

A number of R packages have been created to do power analyses through simulations for a variety of purposes and contexts, including for genetic association studies \citep{Gaye2015}, microbiome studies \citep{Kelly2015}, network data \citep{Handcock2008}, survey statistics \citep{Alfons2010}, ... . More general-purpose R packages also exist for simulations for power analyses, including packages for power analyses for generalized linear mixed models \cite{Green2016}, ... . 

## Using simulated time series to test modeling questions

Simulation studies can also be used to assess different models and modeling choices. This way of assessing models can be particularly useful for complex statistical methods, which can often be very hard to assess performance metrics of using analytical methods \citep{Alfons2010}.

Several R packages exist that simulate data to allow model evaluation in a variety of domains. For example, \code{ss3sim} allows the simulation of fisheries stock assessment for use in evaluating methods in fishery stock assessments \citep{Anderson2014}.

## Challenges in creating simulated time series

For many environmental time series studies, including those investigating air pollution and temperature, both exposures and outcomes can have complex seasonal and longer-term patterns over the study period. For example, air pollution concentrations can change seasonally, both because of changes in emission patterns and meteorological patterns (for example, winter inversion layers can cause a build-up of particulate matter in the winter, while high temperatures in the summer can accelerate ozone formation from precursors); air pollution concentrations can also change over years, especially with evolving Clean Air policies.

Exposures, outcomes, and their relationship with each other can all have complex patterns that could cause problems in epidemiological studies. For example, \citet{Armstrong2014} explain how autocorrelation in temperature data (e.g., days right after a very severe heat wave are likely to be hotter than days right after a milder heat wave) can introduce spurious results in a model testing for evidence of short-term mortality displacement following heat waves.

There are a few important advantages to having an open-source software package that can simulate this type of environmental time series for simulation studies. First, current methods of simulating environmental time series to use to assess modeling choices vary across different studies. While, ideally, results from a simulation study would be robust to different choices in how to simulate the data used in the study, this may not always be the case; if so, results across studies using different methods of simulating data would not be comparable \citep{Alfons2010}. This package offers a tool that both allows different research to simulate environmental time series for simulation studies in a consistent way, it also offers several methods of simulating time series, and so allows researchers to check if simulation results are sensitive to the method of simulating study data. Tools for consistency across studies have helped in the progress of environmental epidemiology research; for example, the public availability of the NMMAPs dataset--which included air pollution, weather, and mortality counts in over 100 US communities for 14 years-- allowed researchers around the world to explore hypotheses and investigate health effects of air pollution and temperature using the same data, and so differences in results could be traced to methodological differences rather than random differences in environmental datasets. [We could add some on the usefulness of (1) reproducible studies and (2) open source software for scientific studies here.]

Further, developing the code to simulate environmental time series in an appropriate way can be time consuming. While researchers often need to customize certain parts of a simulation, to test a particular research question, often most of the simulation can be performed in a way that is generalizable across most other environmental time series studies. Therefore, by providing simulation tools with reasonable defaults, as well as room to customize at each step of the simulation, this package can make environmental epidemiology simulation studies much more efficient and also make simulation a reasonable alternative for determining expected power when planning and proposing future studies.

# Examples of studies that simulated environmental time series

## Mortality displacement

\citet{Armstrong2014} used a simulation study to investigate whether evidence of short-term mortality displacement following heat waves was a result of model artifacts. To do this, they aimed to simulate a time series of heat wave (binary) effects with heat wave effects, none of which were caused by short-term mortality displacement. They then showed that when they fit the model from the previous study to this simulated dataset, the model generated results suggestive of short-term mortality displacement, and less displacement with more severe heat waves than less severe heat waves, which were similar to the findings of the study they were assessing. Overall, this analysis suggested that the earlier study's results might have been an artifact of the modeling approach they used, and so their results could be consistent with a hypothesis of no mortality displacement.

To do this simulation, which required simulating a time series with a heat wave effect on mortality, but no short-term displacement to explain some of that excess mortality, \citet{Armstrong2014} started with a real dataset of daily temperature and mortality counts. For this, they used the NMMAPS dataset (1987--2000) from Atlanta, GA, which was one of the datasets used in the original study. They fit a model of expected log mortality count for this real data regressed on daily (lag 0) temperature and a smooth function of time (to capture seasonal and long-term variation). They then predicted from this model for the days in their simulation, to generate a smoothed estimate of expected daily mortality on each day (note: this daily expected count is a function of observed temperature over the study period; it sounds like they used observed data directly for the exposure time series). They then drew simulated daily mortality counts from a Poisson distribution with $\lambda$ equal to this daily expected mean mortality for each day in the simulation period. [Code they used for this simulation analysis is available as an online supplement.]

## Comparing case-crossover and time series

\citet{Bateson1999} compare several different case-crossover designs in the analysis of data with confounding seasonal trends.  They generated both exposure and response data exhibiting each of eight different temporal trends, which can be done with eesim.  They analyzed the simulated data using five different case-crossover designs, which could be done using the customization features of eesim.  They assessed these models with measures such as mean variance of the estimates, relative efficiency, and coverage probability, all included in eesim.  Based on the model performances, the authors concluded that symmetric bi-directional design with a lag period of one week can control for a large variety of confounding time trends.   

## Investigating confounding

\citet{Peng2006} investigated the effects of confounding factors and model choice on time series studies of air pollution and mortality. In their simulation study, the authors specified four scenarios differing in smoothness and concurvity between pollutant and time trend variable.  They simulated exposure and mortality data based on a data from Minneapolis-St. Paul, for which they could use eesim's customization feature.  They then fit Poisson regression models to each simulated data set, which could be done using eesim.  They assessed the models with measures including AIC and BIC and found that different approaches to smoothing functions of time have different effects on the bias and variance of the effect estimates depending on factors such as the concurvity of the data.  

\citet{Bateson2001} investigated confounding in case-crossover studies.  They simulated continuous exposure data with a cosinusoidal trend and generated Poisson-distributed outcome counts with the same trend, all of which can be done with eesim's functions.  

## Exploring spatial misalignment between exposure and outcome measurements

\citet{Strickland2015} used a simulation study to investigate the effects of measurement error resulting from instrument imprecision and spacial misalignment on the estimates of health effects related to ambient air pollutant concentrations.  They first used air-quality measurements obtained from several networks of monitoring stations in Atlanta to estimate various distributional properties associated with pollutant concentrations, including spatial and temporal autocorrelation. They used these estimates to generate daily two-dimensional pollutant fields for Atlanta at 5 km grids, producing "true" population-weighted average pollutant concentrations (TPWA).  They then simulated grid-specific counts of daily ED visits using TPWA as a covariate, among others.  

The authors then created three different pollutant metrics based on their simulated data which contained additional measurement error and compared estimates of relative risk using these metrics to the RR = 1.05 they specified while simulating the TPWA.  As expected they found that the population-weighted average metric resulted in less bias than measurements from a single monitoring station or measurements averaged across stations.  

\citet{Gryparis2009} compared approaches to fitting health models using predicted exposures featuring spatial misalignment.  The authors considered four levels of heterogeneity in exposure values over a geographic area and simulated exposure data for each.  They simulated both continuous and binary health outcomes, which could be done using eesim, and evaluated several approaches to relating exposure to outcome, including Bayesian and RC-OOS approaches.  Their simulation results suggested that several approaches may be appropriate to discover relationships in spatially misaligned data.

\citet{Butland2013} performed a simulation study to compare models using geographically resolved, simulated air pollution measures and air pollution concentrations averaged over geographic areas.  Recently, regional air pollution chemistry-transport models (CTMs) have been developed which can simulate air pollution concentrations over highly-resolved geographic areas.  These are useful where pollutants cannot be measured or can only be measured in a few locations.  The authors used simulation to investigate if and when it is better to use air pollution measurements from CTMs when estimating health outcomes rather than measures obtained from averaging over geographic grids based on monitor data. 

The authors first simulated "true" values for pollutant concentrations over 100 grid squares, each with a specific mean concentration.  They generated daily values for each grid square over three years, including between-grid covariances.  Then they sampled the number of daily deaths from a poisson with mean dependent on the "true" concentrations.  Their generation of daily mortality counts could be done using eesim.  The authors then simulate  monitor data with classical measurement error and CTM data with both classical and Berkson error.  They separately fit the CTM data and regional averages from the monitor data to the simulated mortality counts and display several measures of model performance, including standard error of the estimates, coverage, and power, all of which can be produced using eesim.  Their results suggested that there may be a significant penalty in the form of increased bias in health effect estimates for the use of CTMs over monitors due to the additional measurement error.  

##PM and Mortality

\citet{Roberts2006} used a simulation study to test the reliability of using AIC to determine the shape of the relationship between particulate matter and mortality.  The authors used Chicago NMMAPS data, which is the default data set in eesim. They estimated the effects of confounders such as temperature on the mortality time series, all from this Chicago NMMAPS, and incorporated the estimates in generating mortality time series with author-specified relationships with PM dose.  These explicitly specified relationships were piecewise linear relationships with and without a lower threshhold for effects.  They could use our customization functions to specify these relationships. Then they estimated the shape of the dose-response relationship using a Poisson log-linear model and four different candidate models for the dose-response relation.  They found the AIC for each of these models and determined whether the AIC was lowest for the models which reflected the true relationship between PM and response in the simulated data.  They found that AIC does not always detect nonlinear relationships in dose-response data.  

##High-dimensional problems

\citet{Bobb2015} used simulation to assess the performance of a proposed method for estimating the health effects of multi-pollutant exposures called Bayesian kernel machine regression (BKMR).  The authors generated exposure data by resampling from two representative data sets.  They then specified several different relationships between response and the components of each pollutant mixture to generate response data, for which they would need to use the customization features in eesim.  They fit kernel machine regression using both frequentist and Bayesian approaches, which would again require customization.  Their model performance measures included standard error of the estimates, power, and coverage, all of which eesim offers.  They found that there are advantages of using BKMR over KMR to detect relationships between mixture pollutants and health.  

# Basic overview of package

[Chart / Figure of functions included and simulation process]

eesim consists of four major parts: generating exposure time series, generating outcome time series, fitting models, and assessing models.  Exposure time series can be generated using the sim\_exposure function with a variety of built-in seasonal trends or using a custom trend specified by the user. This function also allows the user to choose either binary or continuous exposure values.  Using the resulting exposure values as an input, outcomes can then be generated using the sim\_outcome function. For outcomes as well as exposure, the user may specify a built-in seasonal trend or a custom trend.  

The package includes functions for fitting two different models of the exposure and outcome data: a generalized linear model with splines and a case-crossover model.  The function rep\_sims runs many simulations for a given data set and model.  Finally, the function check\_sims returns several measures of model performance including the mean estimate of relative risk, mean variance of the relative risk estimates, percent bias, coverage, and power.  

A time series of simulated count outcome data with Poisson random noise can be simulated using the following model \citep{Johnson2015}:

\begin{equation}
\eta_t = \beta_0 + \sum_{m=1}^{p}\beta_{m}X_{m,t} 
\end{equation}

\begin{equation}
\eta_t = log(\lambda_t)
\end{equation}

\begin{equation}
y_i \sim Pois(\lambda_t)
\end{equation}

\noindent which is a Generalized Linear Model where $\eta_t$ is the linear predictor and a log link is used.

# Implementing the package

## Simulating time series

## Using simulated time series for power analysis

Running a power analysis requires specifications of effect sizes, baseline counts of health outcomes, and shape of seasonal trends in exposures and outcomes. These values can be specified at realistic values based on previous research \citep{Johnson2015}; if no relevant previous research exists, in some cases a pilot study can be used to determine reasonable values to use for these inputs \citep{Johnson2015}.

## Using simulated time series to test modeling questions

## Evaluating simulations based on these time series

A number of different metrics are used to evaluate the results of simulation studies. These include: 

* False positive rate / type I error rate (percent of time significant effect is detected-- i.e., null hypothesis is rejected-- when there is not a true effect) \citep{Bellan2015, Johnson2015}
* Power / Statistical power to detect a true effect of a certain size / empirical power (percent of simulations in which a true effect of a certain size is identified with a p-value of 0.05 or lower; i.e., probability of rejecting the null hypothesis when the null hypothesis is false) \citep{Bellan2015, Johnson2015, Gaye2015, Green2016, Beaujean2014}
* Precision of effect estimate (average confidence interval width over all simulations; some studies give average half-width) \citep{Johnson2015, Beaujean2014}
* Coverage / 95% confidence inverval coverage (percent of 95% confidence intervals from simulations that include the true effect size) \citep{Johnson2015, Beaujean2014}
* Bias in effect estimate / relative parameter estimate bias / relative bias in point estimate \citep{Johnson2015, Beaujean2014, Schoemann2014}
* Relative standard error bias / standard error bias \citep{Beaujean2014, Schoemann2014}
* Margin of error [?] \citep{Johnson2015}
* Sample size required to detect a certain effect size \citep{Burton2009, Gaye2015, Green2016, Beaujean2014, Schoemann2014}
* Time to achieve required number of cases \citep{Burton2009}
* Power curve \citep{Beaujean2014}
* Accuracy curve \citep{Beaujean2014}

\citet{Beaujean2014} gives the following equation for calculating relative parameter estimate bias, given $\theta_{H}$ (true value of the parameter, as set when creating the simulated data) and $\hat{\theta}$ (value of the parameter estimated from applying a model to the simulated data): 

\begin{equation}
\theta_{bias} = \frac{\hat{\theta} - \theta_{H}}{\theta_{H}}
\end{equation}

\citet{Beaujean2014} gives the following equation for calculating relative standard error bias, given $\sigma_{\hat{\theta}}$ (the standard deviation of the parameter estimations across the simulations) and $\hat{\sigma_{\theta}}$ (the average of the standard errors estimated for the simultations):

\begin{equation}
\sigma_{bias} = \frac{\hat{\sigma_{\theta}} - \sigma_{\hat{\theta}}}{\sigma_{\hat{\theta}}}
\end{equation}

## Customizing simulations

While R packages always offer the opportunity for a user to alter the original code to create customized solutions, this process can require extensive skill in R programming for anything but fairly straightforward functions. Here, we provide the opportunity at every step of the simulation process for a user to input custom R functions, from simple to complex, to be run through our functions, without requiring changes to the function code provided by the package. 

For each of the customization options, we have provided the input and output rules required for a custom function to use for that step. A custom function can include any code, and implement any functionality, as long as it adheres to these rules for the input it takes and output it produces. Further, through using R's "dots" functionality, custom functions can include inputs other than those specified by our interface rules, as long as the custom function inputs, at a minimum, the inputs required by the interface rules. 

# Discussion

## Limitations

We currently do not include overdispersion when simulating health outcome counts. \citet{Johnson2015} discusses why this might be problematic and suggests some methods for incorporating overdispersion in a simulation in cases where count outcome data is expected to include overdispersion. They suggest that the following model can be used to generate the expected outcome count with overdispersion-- modeling the linear predictor as: 

\begin{equation}
\eta_t = \beta_0 + \sum_{m=1}^{p}\beta_{m}X_{m,t} + \epsilon_t
\end{equation}

\noindent where $\epsilon_t$ represents the overdispersion and follows a normal distribution: 

\begin{equation}
\epsilon_t \sim N(0, \sigma_{\epsilon}^2)
\end{equation}

In the case of no overdispersion, $\sigma_{\epsilon}^2$ equals 0. In the case of overdispersion, $\sigma_{\epsilon}^2$ will be larger than 0.

# Acknowledgments

# References
